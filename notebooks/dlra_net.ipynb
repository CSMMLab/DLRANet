{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "`Learn the Basics <intro.html>`_ ||\n",
    "**Quickstart** ||\n",
    "`Tensors <tensorqs_tutorial.html>`_ ||\n",
    "`Datasets & DataLoaders <data_tutorial.html>`_ ||\n",
    "`Transforms <transforms_tutorial.html>`_ ||\n",
    "`Build Model <buildmodel_tutorial.html>`_ ||\n",
    "`Autograd <autogradqs_tutorial.html>`_ ||\n",
    "`Optimization <optimization_tutorial.html>`_ ||\n",
    "`Save & Load Model <saveloadrun_tutorial.html>`_\n",
    "\n",
    "Quickstart\n",
    "===================\n",
    "This section runs through the API for common tasks in machine learning. Refer to the links in each section to dive deeper.\n",
    "\n",
    "Working with data\n",
    "-----------------\n",
    "PyTorch has two `primitives to work with data <https://pytorch.org/docs/stable/data.html>`_:\n",
    "``torch.utils.data.DataLoader`` and ``torch.utils.data.Dataset``.\n",
    "``Dataset`` stores the samples and their corresponding labels, and ``DataLoader`` wraps an iterable around\n",
    "the ``Dataset``.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch offers domain-specific libraries such as `TorchText <https://pytorch.org/text/stable/index.html>`_,\n",
    "`TorchVision <https://pytorch.org/vision/stable/index.html>`_, and `TorchAudio <https://pytorch.org/audio/stable/index.html>`_,\n",
    "all of which include datasets. For this tutorial, we  will be using a TorchVision dataset.\n",
    "\n",
    "The ``torchvision.datasets`` module contains ``Dataset`` objects for many real-world vision data like\n",
    "CIFAR, COCO (`full list here <https://pytorch.org/vision/stable/datasets.html>`_). In this tutorial, we\n",
    "use the FashionMNIST dataset. Every TorchVision ``Dataset`` includes two arguments: ``transform`` and\n",
    "``target_transform`` to modify the samples and labels respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass the ``Dataset`` as an argument to ``DataLoader``. This wraps an iterable over our dataset, and supports\n",
    "automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, i.e. each element\n",
    "in the dataloader iterable will return a batch of 64 features and labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]:  torch.Size([64, 1, 28, 28])\n",
      "Shape of y:  torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about `loading data in PyTorch <data_tutorial.html>`_.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Models\n",
    "------------------\n",
    "To define a neural network in PyTorch, we create a class that inherits\n",
    "from `nn.Module <https://pytorch.org/docs/stable/generated/torch.nn.Module.html>`_. We define the layers of the network\n",
    "in the ``__init__`` function and specify how data will pass through the network in the ``forward`` function. To accelerate\n",
    "operations in the neural network, we move it to the GPU if available.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about `building neural networks in PyTorch <buildmodel_tutorial.html>`_.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "torch.Size([5, 3])\n",
      "tensor([[0.2355, 0.3397],\n",
      "        [0.2018, 0.2895],\n",
      "        [0.3089, 0.4439],\n",
      "        [0.2257, 0.3246],\n",
      "        [0.1079, 0.1550]], grad_fn=<AddBackward0>)\n",
      "torch.Size([5, 2])\n"
     ]
    }
   ],
   "source": [
    "# define the network\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "import torch\n",
    "class NormalNet(nn.Module):\n",
    "    weights:list#[torch.Tensor]\n",
    "    acts:list#[torch.nn.ReLU]\n",
    "    biases:list#[torch.Tensor]\n",
    "    num_layers:int \n",
    "\n",
    "    def __init__(self,input_dim:int,output_dim:int,layer_width:int,num_layers:int):\n",
    "        self.num_layers = num_layers\n",
    "        W = torch.rand(input_dim,layer_width)\n",
    "        W = W/torch.norm(W)\n",
    "        self.weights:list = [W]\n",
    "        self.biases:list = [torch.zeros(layer_width)]\n",
    "        self.acts:list=[torch.nn.ReLU()]\n",
    "        for i in range(1,num_layers-1):\n",
    "            W = torch.rand(layer_width,layer_width)\n",
    "            W = W/torch.norm(W)\n",
    "            self.weights.append(W)\n",
    "            self.biases.append(torch.zeros(layer_width))\n",
    "            self.acts.append(torch.nn.ReLU())\n",
    "        W = torch.rand(layer_width,output_dim)\n",
    "        W = W/torch.norm(W)\n",
    "        self.weights.append(W)\n",
    "        self.biases.append(torch.zeros(output_dim))\n",
    "        self.acts.append(torch.nn.Identity()) # last layer with identity output\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0,self.num_layers):\n",
    "                # mark for auto differentiation tape\n",
    "                self.weights[i].requires_grad = True\n",
    "                self.biases[i].requires_grad = True\n",
    "        return None\n",
    "\n",
    "    def forward_pass(self,input_tensor:torch.Tensor)->torch.Tensor:\n",
    "        z =  input_tensor\n",
    "        #self.acts[0](torch.matmul(input_tensor,self.weights[0]) +self.biases[0])\n",
    "        #self.weights[0].requires_grad = True\n",
    "        #self.biases[0].requires_grad = True\n",
    "        for i in range(0,self.num_layers):\n",
    "            self.weights[i].requires_grad = True\n",
    "            self.biases[i].requires_grad = True\n",
    "            z = self.acts[i](torch.matmul(z,self.weights[i]) +self.biases[i])\n",
    "        return z\n",
    "    \n",
    "    def update_step(self,stepsize:float=1e-3):\n",
    "        ## assumes loss.backward() is called\n",
    "        with torch.no_grad():\n",
    "            for i in range(0,self.num_layers):\n",
    "                self.weights[i]=self.weights[i]- stepsize* self.weights[i].grad\n",
    "                self.biases[i]=self.biases[i]-stepsize* self.biases[i].grad\n",
    "        return None\n",
    "    \n",
    "    def clear_grads(self):\n",
    "        #print(\"Clear Grads\")\n",
    "        for i in range(0,self.num_layers):\n",
    "            if self.weights[i].grad is not None:\n",
    "                self.weights[i].grad.data.zero_()\n",
    "            if self.biases[i].grad is not None:\n",
    "                self.biases[i].grad.data.zero_()\n",
    "           \n",
    "        return None\n",
    "    \n",
    "    def print_weights(self):\n",
    "        print(\"weights\")\n",
    "        for i in range(0,self.num_layers):\n",
    "            print(\"layer \"+ str(i))\n",
    "            print(self.weights[i])\n",
    "    \n",
    "    def print_weights_grad(self):\n",
    "        print(\"weights grad\")\n",
    "        for i in range(0,self.num_layers):\n",
    "            print(\"layer \"+ str(i))\n",
    "            print(self.weights[i].grad)\n",
    "            \n",
    "    def print_biases(self):\n",
    "        print(\"biases\")\n",
    "        for i in range(0,self.num_layers):\n",
    "            print(\"layer \"+ str(i))\n",
    "            print(self.biases[i])\n",
    "            \n",
    "    def print_biases_grad(self):\n",
    "        print(\"biases\")\n",
    "        for i in range(0,self.num_layers):\n",
    "            print(\"layer \"+ str(i))\n",
    "            print(self.biases[i].grad)\n",
    "\n",
    "## some sanity checks \n",
    "test_net = NormalNet(3,2,10,4)\n",
    "x = torch.rand(5,3)\n",
    "print(x.size())\n",
    "y = test_net.forward_pass(x)\n",
    "print(y)\n",
    "print(y.size())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and\n",
    "backpropagates the prediction error to adjust the model's parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also check the model's performance against the test dataset to ensure it is learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def test(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            x = nn.Flatten()(X)\n",
    "\n",
    "            pred = model.forward_pass(x)\n",
    "            test_loss += F.nll_loss(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process is conducted over several iterations (*epochs*). During each epoch, the model learns\n",
    "parameters to make better predictions. We print the model's accuracy and loss at each epoch; we'd like to see the\n",
    "accuracy increase and the loss decrease with every epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n",
      "torch.Size([64, 10])\n",
      "loss\n",
      "torch.Size([])\n",
      "tensor(-3.1546, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# get loss function and optimizer\n",
    "batch_size = 64\n",
    "input_dim = 784\n",
    "output_dim = 10\n",
    "layer_width = 10\n",
    "layer_num = 3\n",
    "model = NormalNet(input_dim,output_dim,layer_width,layer_num)\n",
    "#loss_fn =  torch.nn.functional.binary_cross_entropy_with_logits\n",
    "\n",
    "# --- sanity checks ---\n",
    "x = torch.rand(batch_size,input_dim)\n",
    "y = torch.randint(0,2,(batch_size,)) #random labels\n",
    "print(y.size())\n",
    "\n",
    "out = model.forward_pass(x)\n",
    "print(out.size())\n",
    "\n",
    "loss = F.nll_loss(out, y)\n",
    "print(\"loss\")\n",
    "print(loss.size())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, curr_epoch):\n",
    "    size = len(dataloader.dataset)\n",
    "    #model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        x = nn.Flatten()(X)\n",
    "\n",
    "        pred = model.forward_pass(x)\n",
    "        print(\"Loss: \" +str(pred))\n",
    "        loss = F.nll_loss(pred, y)\n",
    "        print(\"Loss: \" +str(loss))\n",
    "\n",
    "        # Backpropagation\n",
    "        #optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #model.print_weights_grad()\n",
    "        #model.print_biases_grad()\n",
    "        #model.print_weights()\n",
    "        #model.print_biases()\n",
    "        #print(\"_____________\")\n",
    "        #\n",
    "        model.update_step(stepsize=1e-3)\n",
    "      \n",
    "        model.clear_grads()\n",
    "        #model.print_weights_grad()\n",
    "        #model.print_biases_grad()\n",
    "\n",
    "        #if batch % 100 == 0:\n",
    "        #    loss, current = loss.item(), batch * len(X)\n",
    "        #    print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Loss: tensor(-0.7304, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.7669, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.7154, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.8188, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.7002, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.7375, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.7223, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.7548, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.7870, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.6941, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.8133, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.8285, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.7840, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.8631, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.7971, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.7649, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.7123, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.7371, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.8529, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.9618, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.9283, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.0574, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.8026, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.8117, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.8151, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.8017, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.8081, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.7974, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.7923, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.8802, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.9626, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.9963, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.9748, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.8800, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.8746, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.9421, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.9051, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.8898, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.8820, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.9746, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.9410, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.9905, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.9091, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.9064, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.0306, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.9824, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.8548, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.9287, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.8493, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.9028, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.8981, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.8589, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.9823, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.8996, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.8767, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.9848, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.0492, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.1383, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.9493, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.9758, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.9712, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.9690, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.9970, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.0285, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.9889, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.9938, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.1050, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.0132, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-0.9472, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.0323, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.2249, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.0771, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.1705, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.2676, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.0934, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.1473, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.1430, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.0121, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.1279, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.2009, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.2028, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.2018, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.1831, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.3368, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.3139, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.3290, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.2412, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.2131, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.2194, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.2695, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.1548, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.1381, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.2046, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.4733, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.4830, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.5087, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.3837, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.3252, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.4083, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.3595, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.3669, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.2359, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.1453, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.4020, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.0549, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.1113, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.0378, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.3683, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.4945, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.3314, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.1739, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.2312, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.1520, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.2452, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.0716, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.1995, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.2639, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.1793, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.2695, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.4138, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.3355, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.3993, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.4783, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.6616, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.5583, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.5802, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.5539, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.3075, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.3753, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.5720, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.5036, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.4591, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.8900, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.7902, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.7045, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.7589, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.6088, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.5984, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.4905, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.6515, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.5332, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.5981, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.3429, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.5328, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.3872, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.6711, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.6530, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.7104, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.8549, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.4950, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.6064, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.9041, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.7665, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.6682, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.6938, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.6402, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.8629, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.7109, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.6070, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.1863, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.3651, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.0623, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.9741, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.0250, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.9253, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.8680, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.6296, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.8045, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.9184, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.9457, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.9591, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.1141, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.2665, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.9445, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.9019, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.9422, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.9114, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.9755, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.9633, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.9193, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.0109, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.9520, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.0299, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.0123, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.0919, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.0365, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.9315, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.3832, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.1555, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.2430, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.2730, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.3044, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(-2.2493, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.2702, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.3350, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.3037, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.4463, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.3597, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.2980, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.0743, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.9671, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.3628, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.6200, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.6464, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.8965, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.5391, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.6754, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.4473, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.4747, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.5126, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.6870, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.8941, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.6920, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.1904, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.1085, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.3463, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.4986, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.5035, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.3600, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.8684, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.9707, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.9740, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.8036, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.8809, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.8241, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.5789, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.5296, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.4431, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.5529, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.7428, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.6031, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.6939, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.7499, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.6541, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.5642, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.9171, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.6393, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.6194, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.7938, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.6496, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.5634, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.7345, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.9201, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.8167, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.9363, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.8857, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.0479, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.8475, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.2856, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.0760, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.3061, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.4851, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.4160, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.4358, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.2414, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.2647, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.2451, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.2818, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.2637, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.0887, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.1660, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.2564, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.1770, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.9580, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.2708, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.3608, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.3989, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.7853, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.7579, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.7504, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.9981, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.4630, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.4798, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.5760, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.1735, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.2925, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.3236, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.4864, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.2654, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.4032, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.8303, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.8314, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.6029, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-4.0075, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-4.1675, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-4.0202, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.7275, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.9738, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.8412, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-4.0392, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.9961, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-4.2111, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.8292, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-4.1924, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-4.0219, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-4.3636, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-4.1967, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.7917, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-4.1030, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3.9337, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5.4750, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-4.5343, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5.2387, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5.0034, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5.1918, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5.3551, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5.2233, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5.1641, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5.2377, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5.3053, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5.1945, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5.1102, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5.1206, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-4.8429, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-4.9873, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5.0264, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-4.9039, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-4.7255, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5.4732, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5.1712, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5.7381, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5.4839, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-6.0161, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5.6389, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5.2758, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5.5828, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5.3453, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-6.2470, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5.4851, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-4.9755, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5.5855, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5.5400, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-6.0526, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5.6634, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5.9523, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-6.1521, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-6.9255, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5.4841, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5.8224, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5.0732, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-7.4113, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-7.0046, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-7.4585, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-7.4042, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-6.4477, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5.9443, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-6.3359, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-6.7393, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-6.3205, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-6.8077, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-6.9060, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-6.5237, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-7.1258, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-7.5487, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-8.3473, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-8.3188, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-7.3572, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-8.1796, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-8.0476, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-8.1698, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-8.9121, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-8.0693, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-7.6709, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-7.9140, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-8.3189, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-8.2521, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-7.6537, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-7.7457, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-7.6311, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-7.1520, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-7.1629, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-8.3996, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-8.6222, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-7.9850, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-8.7508, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-8.9271, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-9.4357, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-8.6905, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-8.7454, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-8.7615, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-8.3821, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-10.0254, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-9.1762, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-8.9446, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-9.5778, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-10.7505, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-9.9871, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-9.9873, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-9.8819, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-7.7828, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-9.7481, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-9.3939, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(-9.7941, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-9.9636, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-9.7414, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-12.8140, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-12.5716, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-11.9693, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-12.0431, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-10.4469, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-10.8821, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-11.5930, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-10.6309, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-9.6331, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-9.5362, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-9.8987, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-12.6278, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-11.8994, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-11.8063, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-12.1109, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-11.4184, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-11.6338, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-11.3169, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-10.8669, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-11.0045, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-10.7304, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-13.2804, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-13.6503, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-14.0920, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-13.9008, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-13.2493, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-12.3925, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-13.1803, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-15.6053, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-15.3077, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-15.0246, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-15.1864, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-13.9972, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-15.2949, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-13.3674, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-14.0740, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-15.6342, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-14.1520, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-13.8757, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-13.6690, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-15.4142, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-17.8943, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-17.6928, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-18.0634, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-18.3772, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-18.8540, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-19.1843, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-18.8688, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-20.5480, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-20.0632, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-20.0484, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-19.7448, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-18.2620, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-18.8085, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-19.9097, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-18.9301, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-21.2729, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-20.7108, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-21.3107, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-17.9596, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-18.4372, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-19.2676, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-24.4686, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-24.3020, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-25.4028, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-24.7297, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-21.1576, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-20.9977, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-22.2708, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-21.2010, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-20.5942, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-20.4024, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-21.3643, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-19.1418, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-24.4710, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-23.8872, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-23.2267, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-24.3081, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-23.7822, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-27.7435, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-28.2313, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-28.1301, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-26.8637, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-21.2612, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-23.3709, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-23.2030, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-29.7459, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-28.6898, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-28.7704, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-28.3823, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-28.1951, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-28.2687, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-27.8743, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-29.6199, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-29.9612, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-28.8333, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-31.5180, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-28.8710, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-29.3921, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-29.7017, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-30.1226, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-32.1975, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-32.8031, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-31.9247, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-36.2737, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-38.7891, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-40.3318, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-38.1932, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-37.4776, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-43.8167, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-42.3806, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-47.1743, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-44.8063, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-39.7452, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-42.1378, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-40.6343, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-38.6697, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-43.1381, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-42.8147, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-48.0201, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-51.4901, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-52.5204, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-49.8417, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-52.8689, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-52.5174, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-50.6251, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-48.5380, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-48.4008, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-48.1944, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-46.8763, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-47.1247, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-55.7372, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-61.9204, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-68.6183, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-63.1737, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-62.2321, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-53.9691, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-62.5503, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-63.8255, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-63.4198, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-60.2627, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-69.4365, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-75.1766, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-72.5947, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-74.8863, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-76.3936, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-74.9447, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-78.2421, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-80.1625, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-80.9703, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-81.9732, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-86.1054, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-96.7299, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-87.9125, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-79.9001, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-84.3378, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-84.5170, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-97.6552, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-98.7445, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-105.9340, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-97.5673, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-95.6162, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-105.0966, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-103.6653, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-108.8011, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-99.8376, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-96.1211, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-100.0504, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-91.4072, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-122.1130, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-148.0651, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-135.2193, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-145.5730, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-124.5097, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-130.4222, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-129.6104, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-172.3222, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-173.5764, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-189.0494, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-189.6042, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-171.3617, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-175.9227, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-167.6245, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-167.8527, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-203.8516, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-214.1030, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-221.8441, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-221.3337, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-206.4255, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-208.0768, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-208.8647, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-193.5461, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(-200.5631, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-199.1659, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-246.3328, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-255.5975, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-250.4062, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-254.8206, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-256.3163, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-241.7137, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-299.4156, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-300.4799, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-292.7506, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-292.1806, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-294.9937, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-312.7617, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-296.8591, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-338.1716, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-382.1375, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-342.8397, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-374.1136, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-386.8075, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-426.5697, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-398.3701, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-403.8324, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-466.8287, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-532.1082, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-541.1689, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-550.1309, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-559.6976, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-575.9723, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-617.8225, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-604.3619, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-687.6490, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-690.5098, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-632.7702, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-552.4142, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-653.7473, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-660.8848, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-784.7636, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-807.9312, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-899.7313, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-900.5690, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-917.7097, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-911.4064, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-923.4347, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1140.5240, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1220.5656, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1253.6294, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1268.8615, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1177.8232, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1272.4775, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1279.5151, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1448.3358, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1426.2880, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1506.1974, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1620.9928, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1743.4117, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1968.4554, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1896.5992, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2173.2666, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2328.6001, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2361.0781, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2277.8613, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2823.1208, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2831.4258, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2502.6265, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2572.4553, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2949.3159, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3783.4138, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-4025.8359, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-4796.2983, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5225.6064, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5546.2622, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5822.3975, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-6425.0459, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-5915.7090, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-6534.5293, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-6998.4209, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-7269.8408, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-10032.9463, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-11645.0586, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-11685.1074, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-13526.9678, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-15044.0391, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-15379.2744, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-15749.5039, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-18185.3145, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-20792.2480, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-26470.6973, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-29865.8477, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-33599.3008, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-37413.6133, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-44552.5625, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-53577.0430, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-58929.5117, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-73646.9922, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-91821.2578, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-112953.0547, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-130517.6172, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-161760.0469, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-198064.8281, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-244380.2969, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-346753.6875, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-427127.0938, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-642249.4375, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-914257.6875, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1444738.1250, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2132397.7500, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-3892339.2500, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-7027453., grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-15267831., grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-38892292., grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.3033e+08, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-7.0163e+08, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-8.9649e+09, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-6.3976e+11, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-2.0279e+15, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-1.5597e+22, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-8.9728e+35, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(-inf, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(nan, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NormalNet' object has no attribute '_modules'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-50f3073b4675>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-4a7415765fae>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(dataloader, model)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnum_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \"\"\"\n\u001b[0;32m-> 1736\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   1714\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training mode is expected to be boolean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1716\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1717\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mchildren\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1601\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mchild\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1602\u001b[0m         \"\"\"\n\u001b[0;32m-> 1603\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1604\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_children\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \"\"\"\n\u001b[1;32m   1620\u001b[0m         \u001b[0mmemo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1621\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1622\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NormalNet' object has no attribute '_modules'"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, t+1)\n",
    "    test(test_dataloader, model)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about `Training your model <optimization_tutorial.html>`_.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
